{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo un Motor de Autodiferenciación\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Antonio-Tresol/ai_workshop_2025_minigrad/blob/main/blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## 0. Introducción\n",
    "\n",
    "Un motor de autodiferenciación es un motor que permite definir funciones arbitrarias y calcular sus derivadas de manera automática.\n",
    "Para ello, un motor de autodiferenciación necesita realizar dos cosas:\n",
    "\n",
    "- construir un grafo que represente la función\n",
    "- saber las dependencias de cada nodo en el grafo\n",
    "- conocer el comportamiento local de cada operación definida en la función\n",
    "\n",
    "Un ejemplo de un motor de autodiferenciación muy famoso es PyTorch Autograd. En este notebook, construiremos un motor de autodiferenciación simple desde cero.\n",
    "\n",
    "Primero, veamos cómo se ve PyTorch Autograd en acción.\n",
    "\n",
    "Este taller esta basado en <a href=\"https://github.com/karpathy/micrograd/tree/master\">micrograd</a> de <a href=\"https://github.com/karpathy\">Andrej Karpathy</a>.\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "Ejecute la celda a continuación para instalar las dependencias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Antonio-Tresol/ai_workshop_2025_minigrad.git\n",
    "!apt -qqq install graphviz\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install graphviz\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ai_workshop_2025_minigrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones auxiliares para visualizar el grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from engine.engine import Value\n",
    "\n",
    "\n",
    "def trace(root: Value) -> tuple[set[Value], set[tuple[Value, Value]]]:\n",
    "    \"\"\"Builds the expression graph, with all its vertices and directed edges.\"\"\"\n",
    "    nodes: set[Value] = set()\n",
    "    edges: set[tuple[Value, Value]] = set()\n",
    "\n",
    "    def dfs_build(v: Value) -> None:\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                dfs_build(child)\n",
    "\n",
    "    dfs_build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def draw_dot(root: Value, format: str = \"svg\", rankdir: str = \"LR\") -> Digraph:\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in [\"LR\", \"TB\"]\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={\"rankdir\": rankdir})\n",
    "    # build all the nodes for the visualization\n",
    "    for n in nodes:\n",
    "        dot.node(\n",
    "            name=str(id(n)),\n",
    "            label=f\"{{ {n.label} | data {n.data:.4f} | grad {n.grad:.4f}}}\",\n",
    "            shape=\"record\",\n",
    "        )\n",
    "        if n._op:  # if the node was produce by an operation,\n",
    "            # we add a node to the visualization to represented\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    # building the edges in the visualization\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Autograd y Nuestro Mini-Grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Autograd es un motor de autodiferenciación que trabaja a nivel de tensores. En pocas palabras, un objeto matemático que contiene valores y dimensiones. Un tensor puede ser un escalar, un vector, una matriz o un tensor de orden superior. PyTorch Autograd construye un grafo dinámico que representa la función y calcula las derivadas de manera automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado: 84.5000\n",
      "df/dx: 91.0000\n",
      "df/dy: 39.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Crear tensores con seguimiento de gradiente\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Operaciones simples\n",
    "z = x**2 + y\n",
    "w = z + y * x\n",
    "\n",
    "# Calcular una función escalar final\n",
    "f = w**2 / 2.0\n",
    "\n",
    "# Imprimir resultado del paso hacia adelante\n",
    "print(f\"Resultado: {f.item():.4f}\")\n",
    "\n",
    "# Calcular gradientes\n",
    "f.backward()\n",
    "\n",
    "# Imprimir los gradientes\n",
    "print(f\"df/dx: {x.grad.item():.4f}\")\n",
    "print(f\"df/dy: {y.grad.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-Grad es un motor de autodiferenciación simple que construiremos en este notebook. Mini-Grad también construirá un grafo que representa la función y calculará las derivadas de manera automática. Sin embargo, Mini-Grad tiene su building block en el nivel de escalares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado: 112.5000\n",
      "df/dx: 90.0000\n",
      "df/dy: 45.0000\n"
     ]
    }
   ],
   "source": [
    "from engine.engine import Value\n",
    "\n",
    "# Crear valores con seguimiento de gradiente\n",
    "x = Value(2.0)\n",
    "y = Value(3.0)\n",
    "\n",
    "# Operaciones simples\n",
    "z = 3 * x + y\n",
    "w = z + y * x\n",
    "\n",
    "# Calcular una función escalar final\n",
    "f = w**2 / 2.0\n",
    "\n",
    "# Imprimir resultado del paso hacia adelante\n",
    "print(f\"Resultado: {f.data:.4f}\")\n",
    "\n",
    "# Calcular gradientes\n",
    "f.backward()\n",
    "\n",
    "# Imprimir los gradientes\n",
    "print(f\"df/dx: {x.grad:.4f}\")\n",
    "print(f\"df/dy: {y.grad:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conceptos Básicos y todo a mano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatizando la diferenciación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementación de Mini-Grad Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Una Red Neuronal Simple con Mini-Grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
